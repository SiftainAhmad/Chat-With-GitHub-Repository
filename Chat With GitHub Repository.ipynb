{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat With GitHub Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "requirements.txt\n",
    "langchain\n",
    "deeplake\n",
    "openai\n",
    "tiktoken\n",
    "langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Do_DQfGxD9i",
    "outputId": "fb0bb53f-d287-488c-b625-342801e6feff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 1)) (0.2.3)\n",
      "Requirement already satisfied: deeplake in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 2)) (3.9.9)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 3)) (1.33.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 5)) (0.1.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (0.1.77)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (2.7.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/requirements.txt (line 1)) (8.3.0)\n",
      "Requirement already satisfied: pillow~=10.2.0 in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (10.2.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (1.34.106)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: humbug>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (4.3.3)\n",
      "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: libdeeplake==0.0.129 in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (0.0.129)\n",
      "Requirement already satisfied: aioboto3>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (13.0.1)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake->-r /content/requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from libdeeplake==0.0.129->deeplake->-r /content/requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r /content/requirements.txt (line 3)) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 3)) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai->-r /content/requirements.txt (line 3)) (4.12.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r /content/requirements.txt (line 4)) (2024.5.15)\n",
      "Requirement already satisfied: aiobotocore[boto3]==2.13.0 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake->-r /content/requirements.txt (line 2)) (2.13.0)\n",
      "Requirement already satisfied: aiofiles>=23.2.1 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake->-r /content/requirements.txt (line 2)) (23.2.1)\n",
      "Requirement already satisfied: botocore<1.34.107,>=1.34.70 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake->-r /content/requirements.txt (line 2)) (1.34.106)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake->-r /content/requirements.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake->-r /content/requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake->-r /content/requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake->-r /content/requirements.txt (line 2)) (0.10.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r /content/requirements.txt (line 3)) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r /content/requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /content/requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain->-r /content/requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain->-r /content/requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r /content/requirements.txt (line 1)) (3.10.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain->-r /content/requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain->-r /content/requirements.txt (line 1)) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r /content/requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r /content/requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r /content/requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake->-r /content/requirements.txt (line 2)) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake->-r /content/requirements.txt (line 2)) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake->-r /content/requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.107,>=1.34.70->aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake->-r /content/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain->-r /content/requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.107,>=1.34.70->aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake->-r /content/requirements.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /content/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the OPENAI_API_KEY & ACTIVELOOP_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QKqXHXNWukOg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='sk-proj-K7dflksdljflsdT3BlbkFJBHmXnXovwv;jhk5OCUFVxIm'\n",
    "\n",
    "os.environ['ACTIVELOOP_TOKEN']='eyJhbklds;kdsfaIjoiSldUIn0.eyJpZCI6InNpZndsfjklsadk;llfdjsnTFk1TTEtVGxEVUdTUDcwTlNEWGRMbzIzcVVWTjJlIn0.'\n",
    "#Don't copy this as i have changed the key ðŸ˜‚ðŸ˜‚ðŸ˜‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-GlXOuExzr6",
    "outputId": "44ba2a18-90cb-4595-e831-338ba78e7974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.77)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloning the GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11Hs6J6F4-8l",
    "outputId": "baa32e50-508f-4384-a267-70e9db74d7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'YouTube-Video-Summarizer-Using-Whisper-LangChain'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (6/6), 27.73 KiB | 709.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SiftainAhmad/YouTube-Video-Summarizer-Using-Whisper-LangChain.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the content of Repository using TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UtK5RDnZ5D_b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# Defining the local root directory for the cloned GitHub repository\n",
    "root_dir = \"/content/YouTube-Video-Summarizer-Using-Whisper-LangChain\" \n",
    "\n",
    "# List to hold documents\n",
    "docs = []\n",
    "# File extensions to filter\n",
    "file_extensions = ['.txt', '.py', '.ipynb','.md']\n",
    "\n",
    "# Walk through the directory\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        file_path = os.path.join(dirpath, file)\n",
    "        # Check if file extension is in the allowed list\n",
    "        if file_extensions and os.path.splitext(file)[1] not in file_extensions:\n",
    "            continue\n",
    "\n",
    "        # Initialize and use the loader for each file\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "            loaded_docs = loader.load_and_split()\n",
    "            docs.extend(loaded_docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7x4D4iVo8tpg",
    "outputId": "26f0daa5-0cbf-4cb1-a6d9-28feda0b77ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"# YouTube Video Summarizer Using Whisper & LangChain\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"### Installing LangChain & OpenAI\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 1,\\n   \"metadata\": {\\n    \"colab\": {\\n     \"base_uri\": \"https://localhost:8080/\"\\n    },\\n    \"id\": \"i5OmcaMXwHaC\",\\n    \"outputId\": \"1d2e5ae4-4711-4ac2-f88a-66f2c22aef16\"\\n   },\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Requirement already satisfied: langchain==0.1.4 in /usr/local/lib/python3.10/dist-packages (0.1.4)\\\\n\",\\n      \"Requirement already satisfied: deeplake in /usr/local/lib/python3.10/dist-packages (3.9.8)\\\\n\",\\n      \"Requirement already satisfied: openai==1.10.0 in /usr/local/lib/python3.10/dist-packages (1.10.0)\\\\n\",\\n      \"Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\\\\n\",\\n      \"Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (6.0.1)\\\\n\",\\n      \"Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.0.30)\\\\n\",\\n      \"Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (3.9.5)\\\\n\",\\n      \"Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (4.0.3)\\\\n\",\\n      \"Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.6.6)\\\\n\",\\n      \"Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.33)\\\\n\",\\n      \"Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.0.20)\\\\n\",\\n      \"Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.1.23)\\\\n\",\\n      \"Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.0.87)\\\\n\",\\n      \"Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.25.2)\\\\n\",\\n      \"Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.7.1)\\\\n\",\\n      \"Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.31.0)\\\\n\",\\n      \"Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (8.3.0)\\\\n\",\\n      \"Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (3.7.1)\\\\n\",\\n      \"Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.10.0) (1.7.0)\\\\n\",\\n      \"Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (0.27.0)\\\\n\",\\n      \"Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (1.3.1)\\\\n\",\\n      \"Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.66.4)\\\\n\",\\n      \"Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.11.0)\\\\n\",\\n      \"Requirement already satisfied: pillow~=10.2.0 in /usr/local/lib/python3.10/dist-packages (from deeplake) (10.2.0)\\\\n\",\\n      \"Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.34.106)\\\\n\",\\n      \"Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\\\\n\",', metadata={'source': '/content/YouTube-Video-Summarizer-Using-Whisper-LangChain/YouTube Video Summarizer Using Whisper & LangChain.ipynb'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgeIk12W9kMC",
    "outputId": "04823302-4887-4129-c8fc-1cb73c2026f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\"We have techniques for that for generating images before actually predicting good images that could fit in the video.\\\\n\",\\n      \"It doesn\\'t work very well.\\\\n\",\\n      \"Or if it works, it doesn\\'t produce internal representations that are particularly good for downstream task like object recognition or something of that time.\\\\n\",\\n      \"So attempting to transfer those SSL method that are successful in LP into the realm of images has not been a big success.\\\\n\",\\n      \"It\\'s been somewhat of a success in audio.\\\\n\",\\n      \"But really the only thing that works in the domain of images is those generating architectures where instead of predicting the image, you predict a representation of the image, right? So you feed.\\\\n\",\\n      \"Let\\'s say one view of a scene to the system, you run it to something on that that computes a representation of it.\\\\n\",\\n      \"And then you take a different view of the same scene, you run it through the same network that produces another representation and you train the system in such a way that those two representations are as close to each other as possible.\\\\n\",\\n      \"And the only thing the systems can agree on is the content of the image so they end up including the content of the image independently of the viewpoint.\\\\n\",\\n      \"The difficulty of making this work is to make sure that when you show two different images, it will produce different representations.\\\\n\",\\n      \"So to make sure that there are informative of the inputs and your system didn\\'t collapse and just produce always the same representation for everything.\\\\n\",\\n      \"But that\\'s the reason why the techniques that have been generative architectures have been successful in LP aren\\'t working so well.\\\\n\",\\n      \"And images is their inability to represent complicated complicated uncertainties if you want.\\\\n\",\\n      \"So now that\\'s for training a system in SSL to learn representations of data.\\\\n\",\\n      \"But what I\\'ve been proposing to do in the position paper I published a few months ago is the idea that we should use SSL to get machines to learn predictive world models.\\\\n\",\\n      \"So basically to predict where the world world is going to evolve.\\\\n\",\\n      \"So predict the continuation of a video, for example.\\\\n\",\\n      \"Possibly predict how it\\'s going to evolve as a consequence of an action that an intelligent agent might take.\\\\n\",\\n      \"Because if we have such a world model in an agent, the agent being capable of predicting what\\'s going to happen as a consequence of its action will be able to plan complex sequence of actions to arrive at a particular goal.\\\\n\",\\n      \"And that\\'s what\\'s missing from all the pretty much all the AI systems that everybody has been working on or has been talking about loudly.\\\\n\",\\n      \"Except for a few people who are working on robotics or it\\'s absolutely necessary.\\\\n\",\\n      \"So some of the interesting work there comes out of the robotics community, the sort of machine learning and robotics committee.\\\\n\",\\n      \"Because there you need to have the skip ability for planning.\\\\n\",\\n      \"And the work that you\\'ve been doing is it possible to build that into a large language model or is it incompatible with the architecture of large language models.\\\\n\",\\n      \"It is compatible with large language models.\\\\n\",\\n      \"And in fact, it might solve some of the problems that we\\'re observing with large language models.\\\\n\",\\n      \"One point is large language models is that when you use them to generate text, you initialize them with a prompt, right? So you type in an initial segment of a text, which could be in the form of a question or something.\\\\n\",\\n      \"And then you hope that it will generate a consistent answer to that text.\\\\n\",\\n      \"And the problem with that is that those systems generate text that sounds fine grammatically, but semantically, but sometimes they make various stupid mistakes.\\\\n\",\\n      \"And those mistakes are due to two things.\\\\n\",', metadata={'source': '/content/YouTube-Video-Summarizer-Using-Whisper-LangChain/YouTube Video Summarizer Using Whisper & LangChain.ipynb'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[5:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the text into chunks using CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G3UZw4np6P5o"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# Initialize the CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "# Split the loaded documents\n",
    "splitted_text = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the first five chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsS37DAv9Fqg",
    "outputId": "f9dbbd94-7f09-4005-cec6-9f11302663c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "page_content='{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"# YouTube Video Summarizer Using Whisper & LangChain\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"### Installing LangChain & OpenAI\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 1,\\n   \"metadata\": {\\n    \"colab\": {\\n     \"base_uri\": \"https://localhost:8080/\"\\n    },\\n    \"id\": \"i5OmcaMXwHaC\",\\n    \"outputId\": \"1d2e5ae4-4711-4ac2-f88a-66f2c22aef16\"\\n   },\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Requirement already satisfied: langchain==0.1.4 in /usr/local/lib/python3.10/dist-packages (0.1.4)\\\\n\",\\n      \"Requirement already satisfied: deeplake in /usr/local/lib/python3.10/dist-packages (3.9.8)\\\\n\",\\n      \"Requirement already satisfied: openai==1.10.0 in /usr/local/lib/python3.10/dist-packages (1.10.0)\\\\n\",\\n      \"Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\\\\n\",\\n      \"Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (6.0.1)\\\\n\",\\n      \"Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.0.30)\\\\n\",\\n      \"Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (3.9.5)\\\\n\",\\n      \"Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (4.0.3)\\\\n\",\\n      \"Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.6.6)\\\\n\",\\n      \"Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.33)\\\\n\",\\n      \"Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.0.20)\\\\n\",\\n      \"Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.1.23)\\\\n\",\\n      \"Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.0.87)\\\\n\",\\n      \"Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.25.2)\\\\n\",\\n      \"Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.7.1)\\\\n\",\\n      \"Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.31.0)\\\\n\",\\n      \"Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (8.3.0)\\\\n\",\\n      \"Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (3.7.1)\\\\n\",\\n      \"Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.10.0) (1.7.0)\\\\n\",\\n      \"Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (0.27.0)\\\\n\",\\n      \"Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (1.3.1)\\\\n\",\\n      \"Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.66.4)\\\\n\",\\n      \"Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.10.0) (4.11.0)\\\\n\",\\n      \"Requirement already satisfied: pillow~=10.2.0 in /usr/local/lib/python3.10/dist-packages (from deeplake) (10.2.0)\\\\n\",\\n      \"Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.34.106)\\\\n\",\\n      \"Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\\\\n\",' metadata={'source': '/content/YouTube-Video-Summarizer-Using-Whisper-LangChain/YouTube Video Summarizer Using Whisper & LangChain.ipynb'}\n",
      "\n",
      "Chunk 2:\n",
      "page_content='\"Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\\\\n\",\\n      \"Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.3.2)\\\\n\",\\n      \"Requirement already satisfied: humbug>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.3.2)\\\\n\",\\n      \"Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from deeplake) (4.3.3)\\\\n\",\\n      \"Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\\\\n\",\\n      \"Requirement already satisfied: libdeeplake==0.0.129 in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.0.129)\\\\n\",\\n      \"Requirement already satisfied: aioboto3>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplake) (13.0.0)\\\\n\",\\n      \"Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.6.0)\\\\n\",\\n      \"Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from libdeeplake==0.0.129->deeplake) (0.3.8)\\\\n\",\\n      \"Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\\\\n\",\\n      \"Requirement already satisfied: aiobotocore[boto3]==2.13.0 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake) (2.13.0)\\\\n\",\\n      \"Requirement already satisfied: aiofiles>=23.2.1 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake) (23.2.1)\\\\n\",\\n      \"Requirement already satisfied: botocore<1.34.107,>=1.34.70 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (1.34.106)\\\\n\",\\n      \"Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (1.14.1)\\\\n\",\\n      \"Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (0.11.0)\\\\n\",\\n      \"Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.3.1)\\\\n\",\\n      \"Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (23.2.0)\\\\n\",\\n      \"Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.4.1)\\\\n\",\\n      \"Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (6.0.5)\\\\n\",\\n      \"Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.9.4)\\\\n\",\\n      \"Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.10.0) (3.7)\\\\n\",\\n      \"Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.10.0) (1.2.1)\\\\n\",\\n      \"Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake) (1.0.1)\\\\n\",\\n      \"Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake) (0.10.1)\\\\n\",\\n      \"Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (3.21.2)\\\\n\",\\n      \"Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (0.9.0)\\\\n\",\\n      \"Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.10.0) (2024.2.2)\\\\n\",' metadata={'source': '/content/YouTube-Video-Summarizer-Using-Whisper-LangChain/YouTube Video Summarizer Using Whisper & LangChain.ipynb'}\n",
      "\n",
      "Chunk 3:\n",
      "page_content='\"Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.10.0) (2024.2.2)\\\\n\",\\n      \"Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.10.0) (1.0.5)\\\\n\",\\n      \"Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.10.0) (0.14.0)\\\\n\",\\n      \"Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.4) (2.4)\\\\n\",\\n      \"Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.4) (23.2)\\\\n\",\\n      \"Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (0.7.0)\\\\n\",\\n      \"Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (2.18.2)\\\\n\",\\n      \"Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (3.3.2)\\\\n\",\\n      \"Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (2.0.7)\\\\n\",\\n      \"Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.4) (3.0.3)\\\\n\",\\n      \"Requirement already satisfied: ppft>=1.7.6.8 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (1.7.6.8)\\\\n\",\\n      \"Requirement already satisfied: pox>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (0.3.4)\\\\n\",\\n      \"Requirement already satisfied: multiprocess>=0.70.16 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (0.70.16)\\\\n\",\\n      \"Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.107,>=1.34.70->aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (2.8.2)\\\\n\",\\n      \"Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (1.0.0)\\\\n\",\\n      \"Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.107,>=1.34.70->aiobotocore[boto3]==2.13.0->aioboto3>=10.4.0->deeplake) (1.16.0)\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"!pip install langchain==0.1.4 deeplake openai==1.10.0 tiktoken\\\\n\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"### yt_dlp : to download videos from YouTube and other video sites\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"####  installing the package from the whisper repository on GitHub by OpenAI\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 2,\\n   \"metadata\": {\\n    \"colab\": {\\n     \"base_uri\": \"https://localhost:8080/\"\\n    },\\n    \"id\": \"-3Y5SPXxr113\",\\n    \"outputId\": \"419a95d2-ea8f-4622-e845-d2bdcad416f8\"\\n   },\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"  Installing build dependencies ... \\\\u001b[?25l\\\\u001b[?25hdone\\\\n\",\\n      \"  Getting requirements to build wheel ... \\\\u001b[?25l\\\\u001b[?25hdone\\\\n\",\\n      \"  Preparing metadata (pyproject.toml) ... \\\\u001b[?25l\\\\u001b[?25hdone\\\\n\",\\n      \"  Building wheel for openai-whisper (pyproject.toml) ... \\\\u001b[?25l\\\\u001b[?25hdone\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"!pip install -q yt_dlp\\\\n\",\\n    \"!pip install -q git+https://github.com/openai/whisper.git\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"### Setting up the \\'OPENAI_API_KEY\\'  &  \\'ACTIVELOOP_TOKEN\\'\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {' metadata={'source': '/content/YouTube-Video-Summarizer-Using-Whisper-LangChain/YouTube Video Summarizer Using Whisper & LangChain.ipynb'}\n",
      "\n",
      "Chunk 4:\n",
      "page_content='\"metadata\": {},\\n   \"source\": [\\n    \"### Setting up the \\'OPENAI_API_KEY\\'  &  \\'ACTIVELOOP_TOKEN\\'\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {\\n    \"id\": \"j_kC-rOaw9_t\"\\n   },\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import os\\\\n\",\\n    \"os.environ[\\'OPENAI_API_KEY\\']=\\'tj-proj-K74dsjsdkkjdlJBHmXnXovdsjdsklfddfsioijm\\'\\\\n\",\\n    \"\\\\n\",\\n    \"os.environ[\\'ACTIVELOOP_TOKEN\\']=\\'eyJhbGciOisdjdksfdjkoiSldUIn0.dsjdsfjfkjdsfjdfiLCJhcGlfa2V5IjoiLXVpdU9rNdskjdkjsjdZnTFk1TTEtVGxEVUdTUDcwTlNEWGRMbzIzcVVWTjJlIn0.\\'\\\\n\",\\n    \"# Note: Don\\'t use the above key, i have changed it ðŸ˜‚ðŸ˜‚\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"### Fuunction to download_mp4_from_youtube\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 4,\\n   \"metadata\": {\\n    \"colab\": {\\n     \"base_uri\": \"https://localhost:8080/\"\\n    },\\n    \"id\": \"ipkBtG10xT-e\",\\n    \"outputId\": \"9242deef-beb9-4a22-fa9e-41cd485de0d3\"\\n   },\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": []\\n    }\\n   ],\\n   \"source\": [\\n    \"import yt_dlp\\\\n\",\\n    \"\\\\n\",\\n    \"def download_mp4_from_youtube(url):\\\\n\",\\n    \"    filename = \\'lecuninterview.mp4\\'\\\\n\",\\n    \"    ydl_opts = {\\\\n\",\\n    \"        \\'format\\': \\'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]\\',\\\\n\",\\n    \"        \\'outtmpl\\': filename,\\\\n\",\\n    \"        \\'quiet\\': True,\\\\n\",\\n    \"    }\\\\n\",\\n    \"\\\\n\",\\n    \"    # Downloading the video file\\\\n\",\\n    \"    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\\\\n\",\\n    \"        result = ydl.extract_info(url, download=True)\\\\n\",\\n    \"# url which is to be downloaded\\\\n\",\\n    \"url = \\\\\"https://www.youtube.com/watch?v=mBjPyte2ZZo\\\\\"\\\\n\",\\n    \"download_mp4_from_youtube(url)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"### Using whisper package to transcribe speech from a video(ASR) .\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 5,\\n   \"metadata\": {\\n    \"colab\": {\\n     \"base_uri\": \"https://localhost:8080/\"\\n    },\\n    \"id\": \"Hb9TdVmOxVVw\",\\n    \"outputId\": \"d94a4f96-08f4-4b5a-a0dd-f8ef7269a067\"\\n   },\\n   \"outputs\": [\\n    {\\n     \"name\": \"stderr\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:01<00:00, 80.9MiB/s]\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"import whisper\\\\n\",\\n    \"\\\\n\",\\n    \"model = whisper.load_model(\\\\\"base\\\\\")\\\\n\",\\n    \"result = model.transcribe(\\\\\"lecuninterview.mp4\\\\\")\\\\n\",\\n    \"# print(result[\\'text\\'])\\\\n\",\\n    \"# Not printing the result as it will take a lot of space\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"### Printing few transcribe text\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 9,\\n   \"metadata\": {\\n    \"colab\": {\\n     \"base_uri\": \"https://localhost:8080/\"\\n    },\\n    \"id\": \"hgTDNk-9tcLu\",\\n    \"outputId\": \"48745a2a-674d-4458-8e67-bb24804c7687\"\\n   },\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Hi, I\\'m Craig Smith and this is I on A On.\\\\n\",\\n      \"This week I talked to Jan LeCoon, one of the seminal figures in deep learning development and a long time proponent of self-supervised learning.\\\\n\",\\n      \"Jan spoke about what\\'s missing in large language models and about his new joint embedding predictive architecture which may be a step toward filling that gap.\\\\n\",\\n      \"He also talked about his theory of consciousness and the potential for AI systems to someday exhibit the features of consciousness.\\\\n\",\\n      \"It\\'s a fascinating conversation that I hope you\\'ll enjoy.\\\\n\",\\n      \"Okay, so Jan, it\\'s great to see you again.\\\\n\",\\n      \"I wanted to talk to you about where you\\'ve gone with so supervised learning since last week spoke.\\\\n\",\\n      \"In particular, I\\'m interested in how it relates to large language models because the large language models really came on stream since we spoke.\\\\n\",\\n      \"In fact, in your talk about JEPA, which is joint embedding predictive architecture.\\\\n\",\\n      \"There you go.\\\\n\",' metadata={'source': '/content/YouTube-Video-Summarizer-Using-Whisper-LangChain/YouTube Video Summarizer Using Whisper & LangChain.ipynb'}\n",
      "\n",
      "Chunk 5:\n",
      "page_content='\"In fact, in your talk about JEPA, which is joint embedding predictive architecture.\\\\n\",\\n      \"There you go.\\\\n\",\\n      \"Thank you.\\\\n\",\\n      \"You mentioned that large language models lack a world model.\\\\n\",\\n      \"I wanted to talk first about where you\\'ve gone with self-supervised learning and where this latest paper stands in your trajectory.\\\\n\",\\n      \"But to start, if you could just introduce yourself and we\\'ll go from there.\\\\n\",\\n      \"Okay, so my name is Jan Le Ka or Jan Le Koon who want to do it in Gilleswee and I\\'m a professor at New York University and at the Quarantine Institute in the Center for Data Science.\\\\n\",\\n      \"And I\\'m also the chief AI scientist at Fair, which is the fundamental AI research lab.\\\\n\",\\n      \"That\\'s what Fair stands for.\\\\n\",\\n      \"Admetta, Neil, Facebook.\\\\n\",\\n      \"So tell me about where you\\'ve gone with self-supervised learning, how the joint embedding predictive architecture fits into your research.\\\\n\",\\n      \"And then if you could talk about how that relates to what\\'s lacking in large language models.\\\\n\",\\n      \"Okay, self-supervised learning has been, has basically brought about a revolution in natural language processing because of their use for pre-training transformer architectures.\\\\n\",\\n      \"And the fact that we use transformer architectures for that is somewhat orthogonal to the fact that we use self-supervised learning.\\\\n\",\\n      \"But the way those systems are trained is that you take a piece of text, you remove some of the words, you replace them by black markers, and then you train the very large neural net to predict the words that are missing.\\\\n\",\\n      \"That\\'s a pre-training phase.\\\\n\",\\n      \"And then in the process of training itself to do so, the system learns good representations of text that you can then use as input to its subsequent downstream task, I don\\'t know, translation or Hitchbitch detection or something like that.\\\\n\",\\n      \"So that\\'s been a career revolution over the last three or four years.\\\\n\",\\n      \"And including in sort of very practical applications, like every sort of type of performing contact moderation systems on Facebook, Google, YouTube, et cetera, use this kind of technique.\\\\n\",\\n      \"And there\\'s all kinds of other applications.\\\\n\",\\n      \"Now, large language models are partially this, but also the idea that you can train those things to just predict the next word in a text.\\\\n\",\\n      \"And if you use that, you can have those system generate text spontaneously.\\\\n\",\\n      \"So there\\'s a few issues with this.\\\\n\",\\n      \"First of all, those things are what\\'s called generative models in the sense that they predict the words, the information that is missing, words in this case.\\\\n\",\\n      \"And the problem with generative models is that it\\'s very difficult to represent uncertain predictions.\\\\n\",\\n      \"So in the case of words, it\\'s easy because we just have the system produce essentially what amounts to a score or a probability for every word in the dictionary.\\\\n\",\\n      \"And so it cannot tell you if the word missing in a sentence like the blank chases the mouse in the kitchen.\\\\n\",\\n      \"It\\'s probably a cat, could be a dog, but it\\'s probably a cat, right? So you have some distribution of probability over all words in the dictionary.\\\\n\",\\n      \"And you can handle uncertainty in the prediction this way.\\\\n\",\\n      \"But then what if you want to apply this to let\\'s say video, right? So you show a video to the system, you remove some of the frames in that video and you train you to predict the frames that I\\'m missing.\\\\n\",\\n      \"For example, predict what comes next in a video and that doesn\\'t work.\\\\n\",\\n      \"And it doesn\\'t work because it\\'s very difficult to train the system to predict an image or whole image.\\\\n\",\\n      \"We have techniques for that for generating images before actually predicting good images that could fit in the video.\\\\n\",\\n      \"It doesn\\'t work very well.\\\\n\",' metadata={'source': '/content/YouTube-Video-Summarizer-Using-Whisper-LangChain/YouTube Video Summarizer Using Whisper & LangChain.ipynb'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(splitted_text[:5]):\n",
    "    print(f\"Chunk {i + 1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the chunks in the DeepLake in the form of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYsMFcPEx9lo",
    "outputId": "3700973c-b18d-4857-c36e-1bede38aea73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "WARNING:langchain_community.vectorstores.deeplake:Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 29 embeddings in 1 batches of size 29:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:27<00:00, 27.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://siftain/langchain_course_chat_with_GitHub', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (29, 1)      str     None   \n",
      " metadata     json      (29, 1)      str     None   \n",
      " embedding  embedding  (29, 1536)  float32   None   \n",
      "    id        text      (29, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2bb3c4d4-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3c68c-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3c722-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3c7a4-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3c81c-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3c894-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3c90c-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3c97a-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3c9e8-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3ca60-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3cace-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3cb3c-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3cba0-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3ccd6-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3cd8a-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3ce2a-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3ceca-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3cf7e-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3cfec-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d050-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d0be-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d122-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d190-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d258-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d2da-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d348-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d3ac-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d424-2882-11ef-964b-0242ac1c000c',\n",
       " '2bb3d488-2882-11ef-964b-0242ac1c000c']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "my_activeloop_org_id = \"siftain\"\n",
    "my_activeloop_dataset_name = \"langchain_course_chat_with_GitHub\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(splitted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "rPnx0r5hyDC4",
    "outputId": "33ffc4d0-c642-4d5a-9179-4f5dc0156039"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The name of the repository is \"YouTube-Video-Summarizer-Using-Whisper-LangChain.\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "# Create a retriever from the DeepLake instance\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Set the search parameters for the retriever\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "# Create a ChatOpenAI model instance\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a RetrievalQA instance from the model and retriever\n",
    "qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# Return the result of the query\n",
    "qa.run(\"What is the name of the repository?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking multiple questions related to repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "Vl0vZD2f1Kn5",
    "outputId": "8468a98d-40ab-40c1-aa0b-2dbd9c2d7ba9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The summarization in this repository is done using different techniques such as map_reduce, stuff, and refine. These techniques involve processing the input text to generate concise summaries either in bullet points or through iterative refinement. The summarization process includes extracting key information from the text and presenting it in a structured and condensed form. The techniques used aim to provide meaningful summaries that capture the essential details of the input text.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"How the summarization is done according to this repository?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "i7qMGSND_iy1",
    "outputId": "87608b14-7f56-4d14-f277-46b47d00dda1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The technologies used in this repository to summarize the YouTube video include LangChain, Whisper, OpenAI, and yt_dlp for downloading the videos.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Name the technologies used in this repository to summarize the youtube video?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ftqSvGKQBNvn",
    "outputId": "b088fe42-d221-4645-c0a5-de4d6e3a964c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I am an AI assistant here to help answer any questions you may have based on the information provided. How can I assist you today?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Who are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "V-Jlwp_lCGEA",
    "outputId": "9fb5d6c5-cbb2-4c3d-8e55-1212130e1125"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The code used to transcribe a YouTube video in the provided context involves setting up the necessary API keys, downloading the video from YouTube, and transcribing the speech from the video using the Whisper package. Additionally, the transcribed text can be saved in a file. Here is an overview of the steps involved:\\n\\n1. Set up the API keys for OpenAI and ActiveLoop.\\n2. Download the YouTube video in MP4 format.\\n3. Transcribe the speech from the video using the Whisper package.\\n4. Print or save the transcribed text.\\n\\nIf you need specific code snippets or details on any of these steps, please let me know.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Give me the code that is used to transcribe the youtube video?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "j8_Ks4SvEnjn",
    "outputId": "1ea3cfbb-4920-449f-a671-64f83a25f4b6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The code snippet for transcribing speech from a video using the Whisper package is as follows:\\n\\n```python\\nimport whisper\\n\\nmodel = whisper.load_model(\"base\")\\nresult = model.transcribe(\"lecuninterview.mp4\")\\n```\\n\\nThis code loads the Whisper model and transcribes the speech from the video file named \"lecuninterview.mp4\".'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"I want to know the code of 'Transcribe the speech from the video using the Whisper package'?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpQU2cuoFDjq",
    "outputId": "cfd0a133-4487-499e-9725-c07203fe3c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the code snippet that transcribes the speech from a video using the Whisper package:\n",
      "\n",
      "```python\n",
      "import whisper\n",
      "\n",
      "model = whisper.load_model(\"base\")\n",
      "result = model.transcribe(\"lecuninterview.mp4\")\n",
      "print(result['text'])\n",
      "```\n",
      "\n",
      "This code initializes a model from the Whisper package and transcribes the speech from the video file named \"lecuninterview.mp4\". The result will contain the transcribed text from the video.\n"
     ]
    }
   ],
   "source": [
    "# For more clear visualisation of the code\n",
    "print(qa.run(\"I want to know the code of 'Transcribe the speech from the video using the Whisper package'?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edju7CboFOYY"
   },
   "source": [
    "# Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
